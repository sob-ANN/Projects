## [Non-Linear Regression](https://github.com/sob-ANN/Projects/blob/main/Machine%20Learning%20From%20Scratch/Non-Linear%20Regression.ipynb)
Polynomial Regression applied to solve a simple problem of projectile motion. Use of Batch/Mini-batch/Stocastic Gradient Descent from scratch and plotting Number of Epochs vs Cost Function plots. Also, using a variable learning rate optimized by Line Search Algorithm

## [Multivariate Regression](https://github.com/sob-ANN/Deep-Learning-APL745-/blob/main/Multivariate%20Linear%20Regression.ipynb)
Multivariate Regression using Gradient Descent to predict house prices.

## [Binary Classification](https://github.com/sob-ANN/Deep-Learning-APL745-/blob/main/Binary%20Classification-Logistic%20Reg.ipynb)
Binary Classification of data containing multiple features which affect rainfall. Using that data we make predictions on a test set whether it will rain tomorrow or not.
We use logistic Regression as our hypothesis function and Binary Cross Entropy loss. Optimized using scipy library.

## [Multi-Class Classification](https://github.com/sob-ANN/Deep-Learning-APL745-/blob/main/OnevRest%20Classification.ipynb)
Multiclass Classification of MNIST Dataset from scratch. Optimization is done using Scipy Library. Binary Cross Entropy is the loss function used with Logistic Sigmoid as our hypothesis function. Target labels are one-hot encoded to be able to perform matrix operations.

## [Softmax Multi-Class Classification](https://github.com/sob-ANN/Deep-Learning-APL745-/blob/main/Softmax%20Classification.ipynb)
Multi-class Classification using softmax on the MNIST Fashion Dataset from scratch. Optimized using Batch Gradient Descent. Also, the accuracy is compared with the One-vs-Rest case.

## [Neural Network from Scratch](https://github.com/sob-ANN/Deep-Learning-APL745-/blob/main/Neural%20Network%20from%20Scratch.ipynb)
A multi-layer Perceptron or Neural Network was implemented using Numpy only. This includes implementing the forward methods and the gradients of each of the layers, including the activations.
This gives a clear understanding of how Pytorch/Tensorflow work in the background.

## [Convolution Neural Network from Scratch](https://github.com/sob-ANN/Deep-Learning-APL745-/tree/main/CNN%20Working)
Convolution Neural Network(CNN) has been written from scratch. This project includes writing the convolution forward pass, maxpool, implementing the softmax activation and writing their gradients so that backprop can be performed. 
