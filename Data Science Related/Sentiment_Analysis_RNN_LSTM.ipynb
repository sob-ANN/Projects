{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "284e73d5",
      "metadata": {
        "id": "284e73d5"
      },
      "source": [
        "**RNN**\n",
        "In this code, we first set the parameters such as the maximum number of words to consider as features (max_features), the maximum sequence length (max_length), batch size, and the number of epochs for training.\n",
        "\n",
        "We then load the IMDb dataset using imdb.load_data(). The dataset is automatically split into training and testing sets. We apply padding to the sequences using sequence.pad_sequences() to ensure all input sequences have the same length.\n",
        "\n",
        "The RNN model is built using tf.keras.Sequential with an embedding layer, a simple RNN layer (tf.keras.layers.SimpleRNN), and a dense layer with sigmoid activation for binary classification.\n",
        "\n",
        "The model is compiled with the RMSprop optimizer, binary cross-entropy loss, and accuracy as the evaluation metric.\n",
        "\n",
        "We train the model using model.fit() on the training data and evaluate its performance on the testing data using model.evaluate()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fc9680a2",
      "metadata": {
        "id": "fc9680a2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "3f444678",
      "metadata": {
        "id": "3f444678"
      },
      "outputs": [],
      "source": [
        "max_features = 5000  # Number of words to consider as features\n",
        "max_length = 250  # Maximum sequence length (words)\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "# Load the IMDb dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0ejVroBEHpP",
        "outputId": "094c299d-9ff1-4557-8036-a1d3936dcc98"
      },
      "id": "L0ejVroBEHpP",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0] #ouputs some numbers which are some kind of tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iCdrUIcENLL",
        "outputId": "4a32160e-797f-46b6-a178-05b5cad08897"
      },
      "id": "4iCdrUIcENLL",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 2,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 2,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 2,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 2,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 2,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0] #positive review"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYWMFWF3EZ-j",
        "outputId": "e66189f8-7fa5-475d-ba48-cdc10681dc2b"
      },
      "id": "aYWMFWF3EZ-j",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "word_index['bad']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvtGsutIEd5N",
        "outputId": "4572aff9-b46b-4dfe-8dbe-e74bee5a7c1e"
      },
      "id": "NvtGsutIEd5N",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_word = dict((value,key) for key,value in word_index.items())\n",
        "def decode(review):\n",
        "  txt = ''\n",
        "  for r in review:\n",
        "    txt += get_word[r]\n",
        "    txt += \" \"\n",
        "\n",
        "  return txt"
      ],
      "metadata": {
        "id": "zWZ3QMXsEuyN"
      },
      "id": "zWZ3QMXsEuyN",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode(x_train[19])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "KG7JMdAzFezA",
        "outputId": "c92ea967-be0c-4ff2-cf95-49a3e431f357"
      },
      "id": "KG7JMdAzFezA",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"the exercise he it is tony falls ask has is found and and david order besides and it age cole watch extraordinary would it racist bad them can of order this community dancer of introduced for at does for all with released movies sometimes to for be war rock made all it coming all well fighting in cheap not his movie high and and actors know bad can and chance 6 and and and in and or weren't to and is got say room and legend like that hand some it of and br about help of you it is over are of straight able of their book an of details have stuff that in not that it her and anyone who so and down course and to and for have big 3 not also global she this of played and there theatre will life are goes in still lion to would to and he and who is success save br being before this television god is nobody to tony ask for into and to and have \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23OVW24xGYhI",
        "outputId": "3382e007-8606-4c81-87b6-7983fbb98ee8"
      },
      "id": "23OVW24xGYhI",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250,)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[19] #seems like a positive review"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5jvwFLtF2n3",
        "outputId": "5de167c9-78fa-4d93-928a-46c626f8c56f"
      },
      "id": "n5jvwFLtF2n3",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9eb70d1e",
      "metadata": {
        "id": "9eb70d1e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Pad sequences to a fixed length because each size is differnt\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53e0f59",
      "metadata": {
        "id": "a53e0f59",
        "outputId": "87beb125-9512-4cd5-8de8-3ea8ee54e93f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 61s 77ms/step - loss: 0.5793 - accuracy: 0.6682 - val_loss: 0.4069 - val_accuracy: 0.8248\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 63s 81ms/step - loss: 0.3682 - accuracy: 0.8416 - val_loss: 0.4093 - val_accuracy: 0.8101\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 70s 89ms/step - loss: 0.2991 - accuracy: 0.8767 - val_loss: 0.4193 - val_accuracy: 0.8175\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.2807 - accuracy: 0.8900 - val_loss: 0.3600 - val_accuracy: 0.8634\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 83s 106ms/step - loss: 0.2279 - accuracy: 0.9100 - val_loss: 0.4651 - val_accuracy: 0.7915\n",
            "782/782 [==============================] - 14s 17ms/step - loss: 0.4651 - accuracy: 0.7915\n",
            "Test loss: 0.4651\n",
            "Test accuracy: 0.7915\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Build the RNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(max_features, 32),\n",
        "    tf.keras.layers.SimpleRNN(32),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(f'Test loss: {loss:.4f}')\n",
        "print(f'Test accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc80dda8",
      "metadata": {
        "id": "cc80dda8"
      },
      "source": [
        "In this code, the first steps are the same as in the RNN example. We set the parameters, load the IMDb dataset, and perform padding on the sequences.\n",
        "\n",
        "The LSTM model is built using tf.keras.Sequential with an embedding layer, an LSTM layer (tf.keras.layers.LSTM), and a dense layer with sigmoid activation for binary classification.\n",
        "\n",
        "The model is compiled with the RMSprop optimizer, binary cross-entropy loss, and accuracy as the evaluation metric.\n",
        "\n",
        "We train the model using model.fit() on the training data and evaluate its performance on the testing data using model.evaluate().\n",
        "\n",
        "Feel free to adjust the parameters and experiment with different architecture variations, such as adding dropout or recurrent dropout, stacking multiple LSTM layers, or using bidirectional LSTM, to further enhance the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2bc2b0",
      "metadata": {
        "id": "bb2bc2b0",
        "outputId": "33698287-beb2-4e7c-a40a-5a85beda51ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 144s 179ms/step - loss: 0.4444 - accuracy: 0.7870 - val_loss: 0.3533 - val_accuracy: 0.8447\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 145s 185ms/step - loss: 0.2932 - accuracy: 0.8821 - val_loss: 0.3037 - val_accuracy: 0.8714\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 149s 190ms/step - loss: 0.2602 - accuracy: 0.8981 - val_loss: 0.3270 - val_accuracy: 0.8725\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 140s 179ms/step - loss: 0.2378 - accuracy: 0.9075 - val_loss: 0.3222 - val_accuracy: 0.8708\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 145s 185ms/step - loss: 0.2206 - accuracy: 0.9151 - val_loss: 0.4134 - val_accuracy: 0.8660\n",
            "782/782 [==============================] - 33s 43ms/step - loss: 0.4134 - accuracy: 0.8660\n",
            "Test loss: 0.4134\n",
            "Test accuracy: 0.8660\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Set the parameters\n",
        "max_features = 5000  # Number of words to consider as features\n",
        "max_length = 250  # Maximum sequence length (words)\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "# Load the IMDb dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(max_features, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(f'Test loss: {loss:.4f}')\n",
        "print(f'Test accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7460218c",
      "metadata": {
        "id": "7460218c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}