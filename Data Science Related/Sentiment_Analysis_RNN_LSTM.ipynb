{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7961390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 0s 0us/step\n",
      "---review with words---\n",
      "['the', 'and', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'and', 'and', 'br', 'villain', 'and', 'and', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'and', 'and', 'concept', 'issue', 'and', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'and', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'and', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'and', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in X_train[6]])\n",
    "print('---label---')\n",
    "print(y_train[6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e60d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Maximum review length: {}'.format(\n",
    "len(max((X_train + X_test), key=len))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73710dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum review length: 14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Minimum review length: {}'.format(\n",
    "len(min((X_test + X_test), key=len))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "869404a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a569da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eb0542c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20568\\2274711880.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15929da",
   "metadata": {},
   "source": [
    "\n",
    "The IMDb dataset used in sentiment analysis contains a collection of movie reviews along with their corresponding sentiment labels. It is a commonly used benchmark dataset for sentiment analysis tasks in natural language processing.\n",
    "\n",
    "Here are some details about the IMDb dataset:\n",
    "\n",
    "Size: The dataset is relatively large and contains a total of 50,000 movie reviews.\n",
    "Split: The dataset is divided into two sets: a training set and a testing set. The training set consists of 25,000 reviews, while the testing set contains the remaining 25,000 reviews.\n",
    "Sentiment Labels: Each movie review in the dataset is labeled with a sentiment label indicating whether the review expresses a positive or negative sentiment. The sentiment labels are binary, where 1 typically represents a positive sentiment, and 0 represents a negative sentiment.\n",
    "Review Text: The dataset includes the text of the movie reviews. These reviews vary in length and can contain multiple sentences or paragraphs.\n",
    "Word Index: The dataset comes preprocessed with a word index, where each word in the reviews is assigned a unique integer index. The word index allows for efficient representation and encoding of the textual data.\n",
    "The IMDb dataset serves as a valuable resource for training and evaluating models for sentiment analysis tasks. It enables researchers and practitioners to develop and compare different approaches, such as RNNs and LSTMs, for accurately predicting sentiment based on textual data.\n",
    "\n",
    "When working with the IMDb dataset, it is important to preprocess the text data appropriately, perform any necessary tokenization, and apply padding to ensure consistent input dimensions for the models. This allows the models to effectively learn and extract sentiment information from the textual reviews.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e73d5",
   "metadata": {},
   "source": [
    "**RNN**\n",
    "In this code, we first set the parameters such as the maximum number of words to consider as features (max_features), the maximum sequence length (max_length), batch size, and the number of epochs for training.\n",
    "\n",
    "We then load the IMDb dataset using imdb.load_data(). The dataset is automatically split into training and testing sets. We apply padding to the sequences using sequence.pad_sequences() to ensure all input sequences have the same length.\n",
    "\n",
    "The RNN model is built using tf.keras.Sequential with an embedding layer, a simple RNN layer (tf.keras.layers.SimpleRNN), and a dense layer with sigmoid activation for binary classification.\n",
    "\n",
    "The model is compiled with the RMSprop optimizer, binary cross-entropy loss, and accuracy as the evaluation metric.\n",
    "\n",
    "We train the model using model.fit() on the training data and evaluate its performance on the testing data using model.evaluate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc9680a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f444678",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000  # Number of words to consider as features\n",
    "max_length = 250  # Maximum sequence length (words)\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# Load the IMDb dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eb70d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pad sequences to a fixed length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a53e0f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 61s 77ms/step - loss: 0.5793 - accuracy: 0.6682 - val_loss: 0.4069 - val_accuracy: 0.8248\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 63s 81ms/step - loss: 0.3682 - accuracy: 0.8416 - val_loss: 0.4093 - val_accuracy: 0.8101\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.2991 - accuracy: 0.8767 - val_loss: 0.4193 - val_accuracy: 0.8175\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.2807 - accuracy: 0.8900 - val_loss: 0.3600 - val_accuracy: 0.8634\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 83s 106ms/step - loss: 0.2279 - accuracy: 0.9100 - val_loss: 0.4651 - val_accuracy: 0.7915\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.4651 - accuracy: 0.7915\n",
      "Test loss: 0.4651\n",
      "Test accuracy: 0.7915\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_features, 32),\n",
    "    tf.keras.layers.SimpleRNN(32),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(f'Test loss: {loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80dda8",
   "metadata": {},
   "source": [
    "In this code, the first steps are the same as in the RNN example. We set the parameters, load the IMDb dataset, and perform padding on the sequences.\n",
    "\n",
    "The LSTM model is built using tf.keras.Sequential with an embedding layer, an LSTM layer (tf.keras.layers.LSTM), and a dense layer with sigmoid activation for binary classification.\n",
    "\n",
    "The model is compiled with the RMSprop optimizer, binary cross-entropy loss, and accuracy as the evaluation metric.\n",
    "\n",
    "We train the model using model.fit() on the training data and evaluate its performance on the testing data using model.evaluate().\n",
    "\n",
    "Feel free to adjust the parameters and experiment with different architecture variations, such as adding dropout or recurrent dropout, stacking multiple LSTM layers, or using bidirectional LSTM, to further enhance the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb2bc2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 144s 179ms/step - loss: 0.4444 - accuracy: 0.7870 - val_loss: 0.3533 - val_accuracy: 0.8447\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 145s 185ms/step - loss: 0.2932 - accuracy: 0.8821 - val_loss: 0.3037 - val_accuracy: 0.8714\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 149s 190ms/step - loss: 0.2602 - accuracy: 0.8981 - val_loss: 0.3270 - val_accuracy: 0.8725\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 140s 179ms/step - loss: 0.2378 - accuracy: 0.9075 - val_loss: 0.3222 - val_accuracy: 0.8708\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 145s 185ms/step - loss: 0.2206 - accuracy: 0.9151 - val_loss: 0.4134 - val_accuracy: 0.8660\n",
      "782/782 [==============================] - 33s 43ms/step - loss: 0.4134 - accuracy: 0.8660\n",
      "Test loss: 0.4134\n",
      "Test accuracy: 0.8660\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Set the parameters\n",
    "max_features = 5000  # Number of words to consider as features\n",
    "max_length = 250  # Maximum sequence length (words)\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# Load the IMDb dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_features, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(f'Test loss: {loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460218c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
